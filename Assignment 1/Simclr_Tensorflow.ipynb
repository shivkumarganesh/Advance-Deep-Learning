{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simclr-Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMruYkSl6Ipkd6qbtal6T2u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivkumarganesh/Advance-Deep-Learning/blob/main/Assignment%201/Simclr_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-iqXCQThYW4",
        "outputId": "b86bfccd-6952-4edb-de91-4e6184cce3da"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzsQSnE9h1pq"
      },
      "source": [
        "# Other imports\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Random seed fixation\n",
        "tf.random.set_seed(666)\n",
        "np.random.seed(666)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH7KXvpniDEy",
        "outputId": "19b50056-0ead-4c20-9a57-07e82837cc6d"
      },
      "source": [
        "# Gather dataset\n",
        "!git clone https://github.com/thunderInfy/imagenet-5-categories"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'imagenet-5-categories'...\n",
            "remote: Enumerating objects: 1532, done.\u001b[K\n",
            "remote: Total 1532 (delta 0), reused 0 (delta 0), pack-reused 1532\u001b[K\n",
            "Receiving objects: 100% (1532/1532), 88.56 MiB | 20.19 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPGl46jEiDg0",
        "outputId": "8cb35442-b075-4597-b696-4a0915ac6e63"
      },
      "source": [
        "# How many training images for SimCLR?\n",
        "train_images = list(paths.list_images(\"imagenet-5-categories/train\"))\n",
        "print(len(train_images))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o2bec4EiFfF"
      },
      "source": [
        "# Augmentation utilities (differs from the original implementation)\n",
        "# Referred from: https://arxiv.org/pdf/2002.05709.pdf (Appendxi A \n",
        "# corresponding GitHub: https://github.com/google-research/simclr/)\n",
        "\n",
        "class CustomAugment(object):\n",
        "    def __call__(self, sample):        \n",
        "        # Random flips\n",
        "        sample = self._random_apply(tf.image.flip_left_right, sample, p=0.5)\n",
        "        \n",
        "        # Randomly apply transformation (color distortions) with probability p.\n",
        "        sample = self._random_apply(self._color_jitter, sample, p=0.8)\n",
        "        sample = self._random_apply(self._color_drop, sample, p=0.2)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def _color_jitter(self, x, s=1):\n",
        "        # one can also shuffle the order of following augmentations\n",
        "        # each time they are applied.\n",
        "        x = tf.image.random_brightness(x, max_delta=0.8*s)\n",
        "        x = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s)\n",
        "        x = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s)\n",
        "        x = tf.image.random_hue(x, max_delta=0.2*s)\n",
        "        x = tf.clip_by_value(x, 0, 1)\n",
        "        return x\n",
        "    \n",
        "    def _color_drop(self, x):\n",
        "        x = tf.image.rgb_to_grayscale(x)\n",
        "        x = tf.tile(x, [1, 1, 1, 3])\n",
        "        return x\n",
        "    \n",
        "    def _random_apply(self, func, x, p):\n",
        "        return tf.cond(\n",
        "          tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
        "                  tf.cast(p, tf.float32)),\n",
        "          lambda: func(x),\n",
        "          lambda: x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMiKLvWKiIsK"
      },
      "source": [
        "# Build the augmentation pipeline\n",
        "data_augmentation = Sequential([Lambda(CustomAugment())])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvvb4uRQiMhn"
      },
      "source": [
        "\n",
        "# Image preprocessing utils\n",
        "@tf.function\n",
        "def parse_images(image_path):\n",
        "    image_string = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, size=[224, 224])\n",
        "\n",
        "    return image"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ6tpWCUiO5u"
      },
      "source": [
        "# Create TensorFlow dataset\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "train_ds = (\n",
        "    train_ds\n",
        "    .map(parse_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    .shuffle(1024)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKYtrrDAiTj-"
      },
      "source": [
        "# Architecture utils\n",
        "def get_resnet_simclr(hidden_1, hidden_2, hidden_3):\n",
        "    base_model = tf.keras.applications.ResNet50(include_top=False, weights=None, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = True\n",
        "    inputs = Input((224, 224, 3))\n",
        "    h = base_model(inputs, training=True)\n",
        "    h = GlobalAveragePooling2D()(h)\n",
        "\n",
        "    projection_1 = Dense(hidden_1)(h)\n",
        "    projection_1 = Activation(\"relu\")(projection_1)\n",
        "    projection_2 = Dense(hidden_2)(projection_1)\n",
        "    projection_2 = Activation(\"relu\")(projection_2)\n",
        "    projection_3 = Dense(hidden_3)(projection_2)\n",
        "\n",
        "    resnet_simclr = Model(inputs, projection_3)\n",
        "\n",
        "    return resnet_simclr"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLD6gNgFiYaq",
        "outputId": "54bcffbb-1fbd-4be1-a917-f1a96ff9b29b"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sthalles/SimCLR-tensorflow/master/utils/helpers.py\n",
        "!wget https://raw.githubusercontent.com/sthalles/SimCLR-tensorflow/master/utils/losses.py"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-25 04:58:36--  https://raw.githubusercontent.com/sthalles/SimCLR-tensorflow/master/utils/helpers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 891 [text/plain]\n",
            "Saving to: ‘helpers.py’\n",
            "\n",
            "helpers.py          100%[===================>]     891  --.-KB/s    in 0s      \n",
            "\n",
            "2021-09-25 04:58:36 (42.1 MB/s) - ‘helpers.py’ saved [891/891]\n",
            "\n",
            "--2021-09-25 04:58:36--  https://raw.githubusercontent.com/sthalles/SimCLR-tensorflow/master/utils/losses.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 891 [text/plain]\n",
            "Saving to: ‘losses.py’\n",
            "\n",
            "losses.py           100%[===================>]     891  --.-KB/s    in 0s      \n",
            "\n",
            "2021-09-25 04:58:36 (27.2 MB/s) - ‘losses.py’ saved [891/891]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSvqvT4RiqAq"
      },
      "source": [
        "from losses import _dot_simililarity_dim1 as sim_func_dim1, _dot_simililarity_dim2 as sim_func_dim2\n",
        "\n",
        "def get_negative_mask(batch_size):\n",
        "    # return a mask that removes the similarity score of equal/similar images.\n",
        "    # this function ensures that only distinct pair of images get their similarity scores\n",
        "    # passed as negative examples\n",
        "    negative_mask = np.ones((batch_size, 2 * batch_size), dtype=bool)\n",
        "    for i in range(batch_size):\n",
        "        negative_mask[i, i] = 0\n",
        "        negative_mask[i, i + batch_size] = 0\n",
        "    return tf.constant(negative_mask)\n",
        "\n",
        "\n",
        "def gaussian_filter(v1, v2):\n",
        "    k_size = int(v1.shape[1] * 0.1)  # kernel size is set to be 10% of the image height/width\n",
        "    gaussian_ope = GaussianBlur(kernel_size=k_size, min=0.1, max=2.0)\n",
        "    [v1, ] = tf.py_function(gaussian_ope, [v1], [tf.float32])\n",
        "    [v2, ] = tf.py_function(gaussian_ope, [v2], [tf.float32])\n",
        "    return v1, v2\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "476GFNhGis3R"
      },
      "source": [
        "@tf.function\n",
        "def train_step(xis, xjs, model, optimizer, criterion, temperature):\n",
        "    with tf.GradientTape() as tape:\n",
        "        zis = model(xis)\n",
        "        zjs = model(xjs)\n",
        "\n",
        "        # normalize projection feature vectors\n",
        "        zis = tf.math.l2_normalize(zis, axis=1)\n",
        "        zjs = tf.math.l2_normalize(zjs, axis=1)\n",
        "\n",
        "        l_pos = sim_func_dim1(zis, zjs)\n",
        "        l_pos = tf.reshape(l_pos, (BATCH_SIZE, 1))\n",
        "        l_pos /= temperature\n",
        "\n",
        "        negatives = tf.concat([zjs, zis], axis=0)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for positives in [zis, zjs]:\n",
        "            l_neg = sim_func_dim2(positives, negatives)\n",
        "\n",
        "            labels = tf.zeros(BATCH_SIZE, dtype=tf.int32)\n",
        "\n",
        "            l_neg = tf.boolean_mask(l_neg, get_negative_mask(BATCH_SIZE))\n",
        "            l_neg = tf.reshape(l_neg, (BATCH_SIZE, -1))\n",
        "            l_neg /= temperature\n",
        "\n",
        "            logits = tf.concat([l_pos, l_neg], axis=1) \n",
        "            loss += criterion(y_pred=logits, y_true=labels)\n",
        "\n",
        "        loss = loss / (2 * BATCH_SIZE)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLbUmlD2jCsx"
      },
      "source": [
        "def train_simclr(model, dataset, optimizer, criterion,\n",
        "                 temperature=0.1, epochs=100):\n",
        "    step_wise_loss = []\n",
        "    epoch_wise_loss = []\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for image_batch in dataset:\n",
        "            a = data_augmentation(image_batch)\n",
        "            b = data_augmentation(image_batch)\n",
        "\n",
        "            loss = train_step(a, b, model, optimizer, criterion, temperature)\n",
        "            step_wise_loss.append(loss)\n",
        "\n",
        "        epoch_wise_loss.append(np.mean(step_wise_loss))\n",
        "        wandb.log({\"nt_xentloss\": np.mean(step_wise_loss)})\n",
        "        \n",
        "        if epoch % 10 == 0:\n",
        "            print(\"epoch: {} loss: {:.3f}\".format(epoch + 1, np.mean(step_wise_loss)))\n",
        "\n",
        "    return epoch_wise_loss, model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMvCSiJWjaW4",
        "outputId": "481ba83d-b268-40f8-e3bb-0790f33e7653"
      },
      "source": [
        "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
        "                                                          reduction=tf.keras.losses.Reduction.SUM)\n",
        "decay_steps = 1000\n",
        "lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
        "    initial_learning_rate=0.1, decay_steps=decay_steps)\n",
        "optimizer = tf.keras.optimizers.SGD(lr_decayed_fn)\n",
        "\n",
        "resnet_simclr_2 = get_resnet_simclr(256, 128, 50)\n",
        "\n",
        "epoch_wise_loss, resnet_simclr  = train_simclr(resnet_simclr_2, train_ds, optimizer, criterion,\n",
        "                 temperature=0.1, epochs=200)\n",
        "\n",
        "with plt.xkcd():\n",
        "    plt.plot(epoch_wise_loss)\n",
        "    plt.title(\"tau = 0.1, h1 = 256, h2 = 128, h3 = 50\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/200 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jRWAiusjf2L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}